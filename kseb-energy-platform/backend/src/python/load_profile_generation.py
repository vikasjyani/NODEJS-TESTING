import sys
import json
import argparse
import logging
import traceback
import uuid
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, Any, List, Optional, Tuple, Union
from datetime import datetime, timedelta

# Assuming shared utils are in a 'shared' directory sibling to this script
# Adjust sys.path if shared_utils are structured differently or installed as a package
try:
    from shared import data_utils, validation
except ImportError:
    # Fallback for direct execution if 'shared' is not in PYTHONPATH
    # This assumes 'shared' is in the same directory or one level up and then 'shared'
    # This is not ideal for robust packaging but helps during development.
    try:
        # Attempt to add parent directory to sys.path to find 'shared'
        current_dir = Path(__file__).resolve().parent
        shared_dir = current_dir / "shared"
        if not shared_dir.exists(): # If not sibling, try parent's sibling
            shared_dir = current_dir.parent / "shared"

        if shared_dir.exists():
            sys.path.insert(0, str(shared_dir.parent)) # Add the common parent of 'python' and 'shared'
            from shared import data_utils, validation
        else:
            raise ImportError("Could not find 'shared' directory for utils.")

    except ImportError as e:
        logging.error(f"Critical: Could not import shared utilities: {e}. Ensure 'shared' module is accessible.")
        # Define dummy functions if shared utils are missing, to allow basic script structure to be parsed
        class data_utils: load_excel_sheet=lambda:None; clean_timeseries_data=lambda:None; save_results_json=lambda:None
        class validation: validate_config_keys=lambda:[]
        # This is not a solution for running, just for parsing the file structure.

# --- Logging Configuration ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

# --- Progress Reporter ---
class ProfileProgressReporter:
    def __init__(self, job_id: str, profile_id_python: str): # profile_id_python is generated by Python
        self.job_id_nodejs = job_id # The ID from Node.js controller for the job
        self.profile_id_python = profile_id_python # The unique ID for the profile being generated by Python
        self.current_progress = 0

    def report(self, progress: float, current_step: str, status_message: str, details: Optional[Dict[str, Any]] = None):
        payload = {
            "jobId": self.job_id_nodejs, # Correlate with Node.js job
            "pythonProfileId": self.profile_id_python, # Python's internal ID for this profile
            "progress": round(min(100, max(0, progress)), 2),
            "step": current_step,
            "status": status_message,
            "details": details or {},
            "timestamp": datetime.now().isoformat()
        }
        print(f"PROGRESS:{json.dumps(payload)}", flush=True)
        logger.debug(f"Reported profile progress: {progress:.2f}% - Step: {current_step} - Status: {status_message}")

# --- Load Profile Generator ---
class LoadProfileGenerator:
    def __init__(self, config: Dict[str, Any], job_id_nodejs: Optional[str] = None):
        self.config = config
        self.profile_id_python = config.get("profile_name") or f"profile_{str(uuid.uuid4())[:8]}"
        self.reporter = ProfileProgressReporter(job_id_nodejs or f"job_{self.profile_id_python}", self.profile_id_python)
        self.method = config.get("method", "base_scaling")
        self.start_year = int(config.get("start_year", datetime.now().year + 1))
        self.end_year = int(config.get("end_year", datetime.now().year + 5))

        self.base_input_dir = Path("inputs") # Expects 'inputs' folder relative to CWD (python script dir)
        self.base_results_dir = Path("results") / "load_profiles" # Expects 'results/load_profiles'
        self.base_results_dir.mkdir(parents=True, exist_ok=True)

        logger.info(f"LoadProfileGenerator initialized for profile: {self.profile_id_python}, method: {self.method}")

    def _validate_config(self) -> List[str]:
        required = ["method", "start_year", "end_year"]
        optional = [
            "profile_name", "base_year", "demand_scenario", "growth_rate", "base_demand",
            "historical_years", "stl_seasonal_period", "stl_trend_smoother",
            "load_factor_improvement", "apply_constraints", "min_load", "max_load",
            "monthly_peaks", "output_resolution", "input_template_file", "timeout"
        ]
        errors = validation.validate_config_keys(self.config, required, optional)
        if self.start_year > self.end_year:
            errors.append("Start year cannot be after end year.")
        if self.method == "base_scaling" and "base_year" not in self.config:
            errors.append("Base year is required for 'base_scaling' method.")
        # Add more method-specific validations
        return errors

    def _load_base_year_hourly_data(self, base_year: int) -> Optional[pd.DataFrame]:
        """Loads and validates hourly load data for the base year."""
        template_file = self.config.get("input_template_file", "load_curve_template.xlsx")
        file_path = self.base_input_dir / template_file

        df = data_utils.load_excel_sheet(file_path, sheet_name=str(base_year), expected_columns=['datetime', 'load'])
        if df is None or df.empty:
            logger.error(f"Could not load or empty data for base year {base_year} from {file_path}")
            return None

        try:
            df['datetime'] = pd.to_datetime(df['datetime'])
            df = df.set_index('datetime').sort_index()
            # Ensure it's hourly and covers a full year (or resample/fill)
            if not (len(df) >= 8700 and pd.infer_freq(df.index) in ['H', '60T']):
                 logger.warning(f"Base year {base_year} data is not full-year hourly. Resampling/padding might be needed. Current freq: {pd.infer_freq(df.index)}, length: {len(df)}")
                 # For simplicity, we'll proceed, but real app might need robust resampling here.
            df['load'] = pd.to_numeric(df['load'], errors='coerce').fillna(method='ffill').fillna(method='bfill')
            return df[['load']] # Return only the load column with datetime index
        except Exception as e:
            logger.error(f"Error processing base year {base_year} data: {e}")
            return None

    def _get_annual_scaling_factors(self) -> Dict[int, float]:
        """Determines annual scaling factors based on demand projections or growth rate."""
        factors = {}
        base_year_for_scaling = int(self.config.get("base_year", self.start_year -1)) # Year whose demand is the baseline (1.0 factor)

        # Option 1: Use a demand forecast scenario
        demand_scenario_name = self.config.get("demand_scenario")
        if demand_scenario_name:
            # This part needs to load results from a demand_projection.py output.
            # For now, simulate or expect a specific file format.
            # Example: scenario_demand_file = Path("results/demand_forecasts") / demand_scenario_name / "forecast_summary.json"
            # with open(scenario_demand_file) as f: annual_demands = json.load(f)['annual_total_demand']
            # This is complex to simulate fully here. Let's assume a simple dict for now.
            logger.warning("Demand scenario loading for scaling factors is simplified/mocked.")
            mock_annual_demands = {y: 1000 * (1 + 0.02 * (y - base_year_for_scaling)) for y in range(self.start_year - 5, self.end_year + 1)} # Mock
            base_demand_val = mock_annual_demands.get(base_year_for_scaling)
            if not base_demand_val: raise ValueError(f"Base year {base_year_for_scaling} demand not found in scenario '{demand_scenario_name}'.")

            for year in range(self.start_year, self.end_year + 1):
                factors[year] = mock_annual_demands.get(year, base_demand_val) / base_demand_val
            return factors

        # Option 2: Use simple growth rate
        growth_rate = float(self.config.get("growth_rate", 0.02)) # Default 2%
        for year in range(self.start_year, self.end_year + 1):
            factors[year] = (1 + growth_rate) ** (year - base_year_for_scaling)
        return factors


    def _generate_profile_base_scaling(self) -> Dict[str, Any]:
        self.reporter.report(5, "setup", "Initializing base scaling method.")
        base_year_config = int(self.config.get("base_year"))

        base_hourly_df = self._load_base_year_hourly_data(base_year_config)
        if base_hourly_df is None:
            return {"error": f"Failed to load base year data for {base_year_config}."}
        self.reporter.report(20, "data_load", "Base year data loaded.")

        scaling_factors = self._get_annual_scaling_factors()
        self.reporter.report(30, "scaling_factors", "Annual scaling factors determined.")

        generated_profiles_data = {} # Store {year: pd.DataFrame}
        num_years_to_gen = self.end_year - self.start_year + 1

        for i, year in enumerate(range(self.start_year, self.end_year + 1)):
            prog = 30 + (i / num_years_to_gen) * 60
            self.reporter.report(prog, f"generate_{year}", f"Generating profile for year {year}.")

            factor = scaling_factors.get(year, 1.0) # Default to 1.0 if year not in factors
            profile_this_year = base_hourly_df.copy()
            profile_this_year['load'] *= factor

            # Adjust datetime index to the current year
            # This assumes base_hourly_df has a complete year's hourly index
            new_index = pd.date_range(start=f'{year}-01-01 00:00:00', end=f'{year}-12-31 23:00:00', freq='H')
            if len(new_index) == len(profile_this_year): # Standard year
                profile_this_year.index = new_index
            elif len(new_index) > len(profile_this_year) and pd.Timestamp(year,1,1).is_leap_year: # Leap year, base was not
                 # Handle leap year carefully: repeat a day or fill, for now, let's just align what we can
                profile_this_year.index = new_index[:len(profile_this_year)] # This might truncate or need padding
                profile_this_year = profile_this_year.reindex(new_index, method='ffill') # Reindex and fill
            else: # Non-leap year, base was leap, or other mismatch
                profile_this_year.index = new_index[:len(profile_this_year)]
                profile_this_year = profile_this_year.reindex(new_index, method='ffill')

            generated_profiles_data[year] = profile_this_year

        self.reporter.report(90, "finalizing", "Finalizing scaled profiles.")
        return self._package_results(generated_profiles_data)

    def _generate_profile_stl_decomposition(self) -> Dict[str, Any]:
        # Placeholder: STL decomposition is more complex.
        # Requires historical data, decomposition, trend/seasonality projection, reconstruction.
        # Needs statsmodels.tsa.seasonal.STL
        self.reporter.report(5, "setup", "Initializing STL decomposition (Not fully implemented).")
        logger.warning("STL Decomposition method is a placeholder and not fully implemented.")
        # Simulate some work
        generated_profiles_data = {}
        for year in range(self.start_year, self.end_year + 1):
            self.reporter.report(10 + (year-self.start_year)*10, f"stl_year_{year}", f"Simulating STL for {year}")
            time.sleep(0.1) # Simulate work
            # Create dummy data for now
            index = pd.date_range(start=f'{year}-01-01', periods=8760, freq='H')
            load = 100 + 20 * np.sin(np.arange(8760) * 2 * np.pi / 8760) + np.random.rand(8760) * 5
            generated_profiles_data[year] = pd.DataFrame({'load': load}, index=index)

        self.reporter.report(90, "finalizing", "Finalizing STL profiles.")
        return self._package_results(generated_profiles_data)

    def _calculate_summary_stats(self, yearly_profiles: Dict[int, pd.DataFrame]) -> Dict[str, Any]:
        """Calculates summary statistics for the generated profiles."""
        overall_summary = {}
        year_summaries = {}
        all_loads_flat = []

        for year, df_year in yearly_profiles.items():
            if 'load' in df_year.columns:
                load_series = df_year['load']
                all_loads_flat.extend(load_series.tolist())
                year_summaries[year] = {
                    "peak_demand_mw": float(load_series.max()),
                    "min_demand_mw": float(load_series.min()),
                    "avg_demand_mw": float(load_series.mean()),
                    "total_energy_gwh": float(load_series.sum() / 1000), # Assuming hourly MW -> GWh
                    "load_factor": float(load_series.mean() / load_series.max()) if load_series.max() > 0 else 0,
                }
        if all_loads_flat:
            overall_summary["grand_total_energy_gwh"] = float(sum(s["total_energy_gwh"] for s in year_summaries.values()))
            overall_summary["overall_peak_mw"] = float(max(s["peak_demand_mw"] for s in year_summaries.values()))

        return {"overall": overall_summary, "yearly": year_summaries}


    def _package_results(self, generated_profiles_data: Dict[int, pd.DataFrame]) -> Dict[str, Any]:
        """Packages results into the final dictionary and saves data."""

        # Convert DataFrames to JSON serializable format (list of dicts)
        serializable_data = {}
        for year, df_year in generated_profiles_data.items():
            df_reset = df_year.reset_index() # ensure datetime is a column
            df_reset['datetime'] = df_reset['datetime'].dt.isoformat()
            serializable_data[str(year)] = df_reset.to_dict(orient='records')

        output_file_name = f"{self.profile_id_python}.json"
        output_path = self.base_results_dir / output_file_name

        full_result_package = {
            "profile_id": self.profile_id_python,
            "method": self.method,
            "generation_time": datetime.now().isoformat(),
            "config_used": self.config,
            "years_generated": list(generated_profiles_data.keys()),
            "statistics": self._calculate_summary_stats(generated_profiles_data),
            "data": serializable_data, # The actual profile data
            "saved_path": str(output_path.resolve()) # Absolute path for Node.js to reference
        }

        # Save the full package to a file
        try:
            with open(output_path, 'w') as f:
                json.dump(full_result_package, f, indent=2)
            logger.info(f"Load profile data saved to: {output_path}")
        except Exception as e:
            logger.error(f"Failed to save profile data to {output_path}: {e}")
            # Decide if this should be a fatal error for the Python script
            return {"error": f"Failed to save profile data: {str(e)}", **full_result_package}


        # The Python script should return a summary, not the full data, to Node.js stdout
        # The full data is in the file.
        summary_for_nodejs = {
            "profile_id": self.profile_id_python,
            "method": self.method,
            "generation_time": full_result_package["generation_time"],
            "years_generated": full_result_package["years_generated"],
            "statistics": full_result_package["statistics"],
            "saved_path": str(output_path.resolve()), # Critical for Node.js to find the file
            "message": f"Profile {self.profile_id_python} generated successfully."
        }
        return summary_for_nodejs


    def generate(self) -> Dict[str, Any]:
        """Main entry point to generate load profiles based on configuration."""
        self.reporter.report(0, "initialization", "Validating configuration...")
        config_errors = self._validate_config()
        if config_errors:
            logger.error(f"Configuration errors: {config_errors}")
            self.reporter.report(100, "validation_failed", "Configuration errors", {"errors": config_errors})
            return {"success": False, "profile_id": self.profile_id_python, "error": "Configuration validation failed.", "details": config_errors}

        self.reporter.report(2, "validated", "Configuration valid.")

        result_summary = {}
        if self.method == "base_scaling":
            result_summary = self._generate_profile_base_scaling()
        elif self.method == "stl_decomposition":
            result_summary = self._generate_profile_stl_decomposition()
        # Add other methods here
        else:
            err_msg = f"Unsupported generation method: {self.method}"
            logger.error(err_msg)
            self.reporter.report(100, "error", "Unsupported method", {"error": err_msg})
            return {"success": False, "profile_id": self.profile_id_python, "error": err_msg}

        if "error" in result_summary:
             self.reporter.report(100, "error", "Generation failed", {"error_details": result_summary["error"]})
             return {"success": False, "profile_id": self.profile_id_python, **result_summary}

        self.reporter.report(100, "completed", "Profile generation complete.")
        return {"success": True, **result_summary}


# --- Main Execution ---
def main():
    parser = argparse.ArgumentParser(description="KSEB Load Profile Generation Module")
    parser.add_argument('--config', type=str, required=True, help="JSON string of the generation configuration.")
    parser.add_argument('--job-id', type=str, help="Optional Node.js Job ID for progress tracking.")

    args = parser.parse_args()
    output_result = {}

    try:
        config_data = json.loads(args.config)
        generator = LoadProfileGenerator(config_data, job_id_nodejs=args.job_id)
        output_result = generator.generate()

    except json.JSONDecodeError as e_json:
        logger.error(f"JSON Decode Error: {e_json.msg}", exc_info=True)
        output_result = {"success": False, "error": f"Invalid JSON in --config: {e_json.msg}"}
        sys.exit(1)
    except Exception as e_main:
        logger.error(f"Unhandled Exception in Load Profile Generation: {str(e_main)}", exc_info=True)
        output_result = {"success": False, "error": str(e_main), "traceback": traceback.format_exc()}
        sys.exit(1)
    finally:
        print(json.dumps(output_result, default=str))

if __name__ == "__main__":
    main()
